from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import tensorflow as tf

df = pd.read_csv('https://raw.githubusercontent.com/pykwon/python/master/testdata_utf8/diabetes.csv', header=None)
print(df.head(5))
print(df.info())
print(df.isna().sum()) # 결측치 없음
print(df.iloc[:, 8].unique()) # [0 1] 이항분류임
data = df.values
x = data[:, 0:7]
y = data[:, -1]
# print(feature)
# print(label)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 12)
print(x_train.shape, x_test.shape)
model = Sequential()
model.add(Dense(30, input_dim = 12, activation = 'relu')) # relu, elu, sigmoid
#model.add(tf.keras.layers.BatchNormalization()) # 배치 정규화 - Gradient loss 등의 문제 해결
model.add(Dense(15, activation = 'relu'))
model.add(Dense(8, activation = 'relu'))
model.add(Dense(1, activation = 'sigmoid')) # hidden이 아니고 이항분류기 때문에 sigmoid

# model.add(Dense(10, input_dim = 1, activation = 'relu'))
# model.add(Dense(5, activation = 'relu'))
# model.add(Dense(1, activation = 'sigmoid'))

#opti = tf.keras.optimizers.Adam(0.01)
model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

#early_stop = EarlyStopping(monitor='loss', patience=5) # 학습 조기종료
# , callbacks = [early_stop]
history = model.fit(x_train, y_train, validation_split= 0.3, batch_size = 1, verbose = 1)

#loss, acc = model.evaluate(x_test, y_test, batch_size = 32, verbose=1)
#print("예측정확도 : {}%".format(acc * 100))









