# -*- coding: utf-8 -*-
"""tf_classifi14_dogcat_TL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aaOYrAhL9ZhFlI1FGeDkxhLV2y8LAYlT
"""

import tensorflow_datasets as tfds
import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

(raw_train, raw_validation, raw_test), metadata = tfds.load('cats_vs_dogs', split=['train[:80%]','train[80%:90%]','train[90%:]'], with_info=True, as_supervised = True)   # 8:1:1

print(raw_train)
print(raw_validation)
print(raw_test)

print(metadata)

get_label_name = metadata.features['label'].int2str
print(get_label_name)

for image, label in raw_train.take(10):
    plt.figure()
    plt.imshow(image)
    plt.title(get_label_name(label))
    plt.show()

IMG_SIZE = 160   # All images will be resized to 160 by160

def format_example(image, label):
    image = tf.cast(image, tf.float32) # 이미지 데이터 실수화
    image = (image/127.5) - 1 # 정규화
    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
    return image, label

train = raw_train.map(format_example)
validation = raw_validation.map(format_example)
test = raw_test.map(format_example)
print(train)

# 4. 이미지 셔플링 배칭
BATCH_SIZE = 32
SHUFFLE_BUFFER_SIZE = 1000

train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
validation_batches = validation.batch(BATCH_SIZE)
test_batches = test.batch(BATCH_SIZE)
# 학습 데이터는 임의로 셔플하고 배치 크기를 정하여 배치로 나누어준다.

for image_batch, label_batch in train_batches.take(1):
    pass

print(image_batch.shape)    # [32, 160, 160, 3]

# 5. 베이스 모델 생성 : 전이학습에서 사용할 베이스 모델은 Google에서 개발한 MobileNet V2 모델 사용.
IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)

# Create the base model from the pre-trained model MobileNet V2
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')

feature_batch = base_model(image_batch)
print(feature_batch.shape)   # (32, 5, 5, 1280)

# 6. 계층 동결
base_model.trainable = False
print(base_model.summary())  # Let's take a look at the base model architecture

# 7. 분류 모델링
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_average = global_average_layer(feature_batch)
print(feature_batch_average.shape)  # (32, 1280)

prediction_layer = tf.keras.layers.Dense(1)
prediction_batch = prediction_layer(feature_batch_average)
print(prediction_batch.shape)         # (32, 1)

model = tf.keras.Sequential([
    base_model,
    global_average_layer,
    prediction_layer
])

# 8. 학습 컴파일
base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
# 손실함수는 이진분류이므로 binary cross entropy 사용. 내부 인자값으로 from_logits를 True로 해준다. 기본값 False

# from_logits에 대해서는 hwiyong.tistory.com/335 참조.

print(model.summary())

# 9. 현재 모델 확인
validation_steps=20
loss0, accuracy0 = model.evaluate(validation_batches, steps=validation_steps)

print("initial loss: {:.2f}".format(loss0))              # initial loss: 0.65
print("initial accuracy: {:.2f}".format(accuracy0))  # initial accuracy: 0.55

# 10. 모델 학습  : 시간이 다소 걸림
initial_epochs = 2
history = model.fit(train_batches, epochs=initial_epochs, validation_data=validation_batches)

# Commented out IPython magic to ensure Python compatibility.
# 11. 학습 커브 확인
# %matplotlib inline
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
#plt.show()

# 12. 파인 튜닝
base_model.trainable = True

# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model.layers))  # 155

fine_tune_at = 100  # Fine-tune from this layer onwards

# Freeze all the layers before the 'fine_tune_at' layer
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False
    
# 13. 모델 컴파일  - 미세조정을 위해 학습률만 1/10 한다.
model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate / 10),
              metrics=['accuracy'])
print(model.summary())

# 14. 파인 튜닝 학습
fine_tune_epochs = 2
total_epochs=  initial_epochs + fine_tune_epochs

history_fine = model.fit(train_batches, epochs=total_epochs,
    initial_epoch=  history.epoch[-1],
    validation_data=validation_batches)

#582/582 []-770s 1s/step - loss: 0.0076 - accuracy: 0.9974 - val_loss: 0.059 - val_accuracy: 0.985

# 시각화
acc += history_fine.history['accuracy']
val_acc += history_fine.history['val_accuracy']
loss += history_fine.history['loss']
val_loss += history_fine.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0.8, 1])
plt.plot([initial_epochs-1,initial_epochs-1], plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 1.0])
plt.plot([initial_epochs-1,initial_epochs-1], plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.xlabel('epoch')
#plt.show()